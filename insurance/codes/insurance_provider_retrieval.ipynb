{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09770488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83e85706",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Providers_Network_update.csv')\n",
    "real_list = df[\"In-network Provider\"].dropna().str.strip().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6101a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/cptaswadu/RESCUE-n8n/insurance'\n",
    "load_dotenv(dotenv_path='.env') \n",
    "openai_api_key = os.getenv(\"OPEN_AI_API_KEY\")\n",
    "perplexity_api_key = os.getenv(\"PERPLEXITY_API_KEY\")\n",
    "client = OpenAI(api_key=openai_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "927fadad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_provider(name): #add annotation\n",
    "    '''\n",
    "    Normalize the provider name by removing unnecessary parts and standardizing the format.\n",
    "    1. Kansas City -> Kansas\n",
    "    2. State Medicaid (FFS) -> State Medicaid\n",
    "    3. Blue Shield of -> BS\n",
    "    '''\n",
    "\n",
    "    name = name.strip()\n",
    "\n",
    "    if \"Kansas City\" in name:\n",
    "        name = name.replace(\"Kansas City\", \"Kansas\")\n",
    "    \n",
    "    if name.endswith(\"(FFS)\"):\n",
    "        name = name.replace(\"(FFS)\", \"\").strip()\n",
    "\n",
    "    if \"Blue Shield of\" in name:\n",
    "        name = name.replace(\"Blue Shield of\", \"BS\")\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49718966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_llm_provider_performance(messages, ground_truth_list, prompt_name=None, experiment_id=None, output_dir=\"results/providers_generation_eval_results\"):\n",
    "    \"\"\"\n",
    "    Evaluate the LLM-generated list of in-network providers against a ground truth list.\n",
    "    Saves evaluation results and handles error cases such as invalid JSON responses.\n",
    "\n",
    "    Parameters:\n",
    "    - messages: list of dicts (Chat API message format)\n",
    "    - ground_truth_list: list of strings (manually verified provider names)\n",
    "    - prompt_name: str, identifier for the prompt used\n",
    "    - experiment_id: int or str, identifier for the experiment repetition\n",
    "    - output_dir: str, directory to save evaluation results\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary containing evaluation summary (precision, recall, etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    def try_llm_response_with_retry(message_block, max_retries=3):\n",
    "        \"\"\"\n",
    "        Retry LLM API call up to max_retries times.\n",
    "        \"\"\"\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                print(f\"üîÅ Attempt {attempt} to get LLM response...\")\n",
    "                response = client.responses.create(\n",
    "                    model=\"gpt-4o\",\n",
    "                    tools=[{\"type\": \"web_search_preview\"}],\n",
    "                    input=message_block\n",
    "                )\n",
    "                return response.output_text.strip()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Attempt {attempt} failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 1. Query LLM\n",
    "    response_text = try_llm_response_with_retry(messages)\n",
    "    if not response_text:\n",
    "        return {\n",
    "            \"error\": \"All attempts failed.\",\n",
    "            \"Precision (%)\": 0,\n",
    "            \"Recall (%)\": 0\n",
    "        }\n",
    "\n",
    "    # 2. Preprocess and patch invalid JSON edge cases\n",
    "    response_text = re.sub(r\"^```json\\s*\", \"\", response_text)\n",
    "    response_text = re.sub(r\"\\s*```$\", \"\", response_text)\n",
    "\n",
    "    if response_text.endswith(\",\"):\n",
    "        response_text = response_text.rstrip(\",\") + \"]}\"\n",
    "\n",
    "    elif response_text.endswith(\"[\"):\n",
    "        response_text += \"]}\"\n",
    "\n",
    "    elif \"Providers\" in response_text and \"source_url\" not in response_text:\n",
    "        response_text += ', \"source_url\": \"\"}'\n",
    "\n",
    "    # 3. Attempt to parse JSON\n",
    "    try:\n",
    "        result = json.loads(response_text)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"‚ùå JSON decoding failed even after retry.\")\n",
    "        print(response_text[:500])\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        fail_path = os.path.join(output_dir, f\"{prompt_name}_experiment{experiment_id}_failed.csv\")\n",
    "        pd.DataFrame([{\"error\": \"invalid JSON\", \"raw_output\": response_text}]).to_csv(fail_path, index=False)\n",
    "        print(f\"‚ö†Ô∏è Raw output saved to '{fail_path}'\")\n",
    "\n",
    "        return {\n",
    "            \"error\": \"invalid JSON\",\n",
    "            \"Precision (%)\": 0,\n",
    "            \"Recall (%)\": 0\n",
    "        }\n",
    "\n",
    "    # 4. Normalize and compare providers\n",
    "    llm_raw_list = result.get(\"Providers\", [])\n",
    "    llm_normalized_list = [normalize_provider(name) for name in llm_raw_list]\n",
    "\n",
    "    ground_truth_set = set(ground_truth_list)\n",
    "    llm_set = set(llm_normalized_list)\n",
    "\n",
    "    common = ground_truth_set & llm_set\n",
    "    missing = ground_truth_set - llm_set\n",
    "    extra = llm_set - ground_truth_set\n",
    "\n",
    "    precision = len(common) / len(llm_set) * 100 if llm_set else 0\n",
    "    recall = len(common) / len(ground_truth_set) * 100 if ground_truth_set else 0\n",
    "\n",
    "    evaluation_result = {\n",
    "        \"prompt_name\": prompt_name,\n",
    "        \"experiment_id\": experiment_id,\n",
    "        \"ground_truth_count\": len(ground_truth_list),\n",
    "        \"llm_returned_count\": len(llm_set),\n",
    "        \"common_count\": len(common),\n",
    "        \"missing_count\": len(missing),\n",
    "        \"extra_count\": len(extra),\n",
    "        \"Precision (%)\": round(precision, 2),\n",
    "        \"Recall (%)\": round(recall, 2)\n",
    "    }\n",
    "\n",
    "    # 5. Save results to CSV\n",
    "    if prompt_name and experiment_id is not None:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        eval_path = os.path.join(output_dir, f\"{prompt_name}_experiment{experiment_id}.csv\")\n",
    "        pd.DataFrame([evaluation_result]).to_csv(eval_path, index=False)\n",
    "        print(f\"üìÅ Evaluation result saved to '{eval_path}'\")\n",
    "\n",
    "    return evaluation_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d372ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def provider_task_prompt_baseline():\n",
    "    '''\n",
    "    The first basic prompt\n",
    "    '''\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant. Respond only in strict JSON format with no explanation or extra commentary.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"List all the medical insurance providers that are currently in-network with GeneDx. \"\n",
    "                \"Format your response as: \"\n",
    "                \"{\\\"Providers\\\": [list of provider names], \\\"source_url\\\": \\\"link to the official source\\\"}. \"\n",
    "                \"Only use information from official GeneDx or trusted affiliate websites.\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "def provider_task_prompt_counted_311():\n",
    "    '''\n",
    "    The second prompt with a specific request for 311 providers\n",
    "    '''\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an artificial intelligence assistant and you need to \"\n",
    "                \"engage in a helpful, detailed, polite conversation with a user.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"List all the 311 medical insurance providers that are currently in-network with GeneDx. \"\n",
    "                \"Output the result strictly in JSON format using the following structure: \"\n",
    "                \"{\\\"Providers\\\": [list of provider names], \\\"source_url\\\": \\\"link to the official source\\\"}. \"\n",
    "                \"Only include links from the official GeneDx website or affiliated trusted sources. \"\n",
    "                \"Do not include any introduction, explanation, or extra commentary ‚Äî only return the JSON object.\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "def provider_task_prompt_explicit_source():\n",
    "    '''\n",
    "    The third prompt with an explicit source\n",
    "    '''\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an artificial intelligence assistant and you need to \"\n",
    "                \"engage in a helpful, detailed, polite conversation with a user.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"List all the medical insurance providers that are currently in-network with GeneDx. \"\n",
    "                \"You may use the official GeneDx insurance network page at \"\n",
    "                \"https://www.genedx.com/commercial-insurance-in-network-contracts/ as the primary source of information. \"\n",
    "                \"Output the result strictly in JSON format using the following structure: \"\n",
    "                \"{\\\"Providers\\\": [list of provider names], \\\"source_url\\\": \\\"link to the official source\\\"}. \"\n",
    "                \"Only include links from the official GeneDx website or affiliated trusted sources. \"\n",
    "                \"Do not include any introduction, explanation, or extra commentary ‚Äî only return the JSON object.\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "prompt_bank = {\n",
    "    \"baseline\": provider_task_prompt_baseline,\n",
    "    \"counted_311\": provider_task_prompt_counted_311,\n",
    "    \"explicit_source\": provider_task_prompt_explicit_source,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7372e94d",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af7303c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Attempt 1 to get LLM response...\n",
      "üìÅ Evaluation result saved to 'results/providers_generation_eval_results/explicit_source_experiment1.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prompt_name': 'explicit_source',\n",
       " 'experiment_id': 1,\n",
       " 'ground_truth_count': 311,\n",
       " 'llm_returned_count': 165,\n",
       " 'common_count': 53,\n",
       " 'missing_count': 257,\n",
       " 'extra_count': 112,\n",
       " 'Precision (%)': 32.12,\n",
       " 'Recall (%)': 17.1}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_llm_provider_performance(\n",
    "            messages=provider_task_prompt_explicit_source(),\n",
    "            ground_truth_list=real_list,\n",
    "            prompt_name=\"explicit_source\",\n",
    "            experiment_id=1\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
