{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "697f1b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cptaswadu/RESCUE-n8n/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import glob\n",
    "import PyPDF2 \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from PyPDF2 import PdfReader\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08084eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/cptaswadu/RESCUE-n8n/insurance'\n",
    "load_dotenv(dotenv_path=os.path.join(path, \".env\"))\n",
    "openai_api_key = os.getenv(\"OPEN_AI_API_KEY\")\n",
    "perplexity_api_key = os.getenv(\"PERPLEXITY_API_KEY\")\n",
    "chatgpt_client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5292e787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPolicyRetriever:\n",
    "    def __init__(self, policy_folder_path, openai_api_key=None, perplexity_api_key=None, llm_model=\"chatgpt\"):\n",
    "        self.policy_folder_path = policy_folder_path\n",
    "        self.policies = {}\n",
    "        self.embeddings = {}\n",
    "        self.embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        self.llm_model = llm_model\n",
    "        self.openai_client = OpenAI(api_key=openai_api_key) if openai_api_key else None\n",
    "        self.perplexity_api_key = perplexity_api_key\n",
    "\n",
    "    def load_policies(self):\n",
    "        pdf_files = glob.glob(os.path.join(self.policy_folder_path, \"*.pdf\"))\n",
    "        for pdf_file in pdf_files:\n",
    "            with open(pdf_file, \"rb\") as f:\n",
    "                reader = PyPDF2.PdfReader(f)\n",
    "                text = \"\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "            self.policies[os.path.basename(pdf_file)] = text\n",
    "        print(f\"‚úÖ Loaded {len(self.policies)} policies.\")\n",
    "\n",
    "    def embed_policies(self):\n",
    "        for doc_name, doc_text in self.policies.items():\n",
    "            self.embeddings[doc_name] = self.embedder.encode([doc_text])[0]\n",
    "        print(\"‚úÖ Embeddings created.\")\n",
    "\n",
    "    def extract_insurance_and_test(self, patient_info):\n",
    "        prompt = f\"\"\"You are a clinical insurance assistant.\n",
    "\n",
    "Given the patient information below, identify:\n",
    "1. The insurance provider (e.g., United Healthcare, Aetna, Medicaid, etc.)\n",
    "2. The type of genetic test (e.g., Whole Exome Sequencing, BRCA1/2, Panel, etc.)\n",
    "3. States where the patient resides (e.g., California, New York, etc.)\n",
    "\n",
    "Respond in JSON format with keys 'insurance', 'test', 'states'.\n",
    "\n",
    "PATIENT INFORMATION:\n",
    "{patient_info}\n",
    "\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "\n",
    "        if self.llm_model == \"chatgpt\":\n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=messages,\n",
    "                temperature=0\n",
    "            )\n",
    "            output = response.choices[0].message.content.strip()\n",
    "\n",
    "        elif self.llm_model == \"perplexity\":\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {self.perplexity_api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            data = {\n",
    "                \"model\": \"sonar-pro\",\n",
    "                \"messages\": messages,\n",
    "                \"temperature\": 0\n",
    "            }\n",
    "            url = \"https://api.perplexity.ai/chat/completions\"\n",
    "            res = requests.post(url, headers=headers, json=data)\n",
    "            output = res.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported LLM model\")\n",
    "\n",
    "        try:\n",
    "            output_json = json.loads(re.sub(r\"```json|```\", \"\", output).strip())\n",
    "            insurance = output_json.get(\"insurance\", None)\n",
    "            test_name = output_json.get(\"test\", None)\n",
    "            return insurance, test_name\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùó JSON parsing error in extract_insurance_and_test: {e}\")\n",
    "            return None, None\n",
    "\n",
    "    def filter_policies_by_insurance(self, insurance_name):\n",
    "        filtered = {}\n",
    "        for doc_name, doc_text in self.policies.items():\n",
    "            if insurance_name and insurance_name.replace(\" \", \"\").lower() in doc_name.replace(\" \", \"\").lower():\n",
    "                filtered[doc_name] = doc_text\n",
    "        return filtered\n",
    "\n",
    "    def find_top_policies(self, patient_info, insurance_name, top_k=5):\n",
    "        filtered_policies = self.filter_policies_by_insurance(insurance_name)\n",
    "        if not filtered_policies:\n",
    "            print(\"‚ùó No policies matched the insurance. Using all policies.\")\n",
    "            filtered_policies = self.policies\n",
    "\n",
    "        query_embedding = self.embedder.encode([patient_info])[0]\n",
    "        scored_policies = []\n",
    "        for doc_name, doc_text in filtered_policies.items():\n",
    "            doc_embedding = self.embeddings[doc_name]\n",
    "            score = cosine_similarity([query_embedding], [doc_embedding])[0][0]\n",
    "            scored_policies.append((doc_name, score, doc_text))\n",
    "\n",
    "        scored_policies.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scored_policies[:top_k]\n",
    "\n",
    "    def rerank_policies(self, patient_info, candidates):\n",
    "        candidate_texts = [c[2][:500].replace(\"\\n\", \" \") for c in candidates]\n",
    "\n",
    "        prompt = f\"\"\"You are an expert insurance policy assistant.\n",
    "\n",
    "You will be given patient information and a list of candidate insurance policies.\n",
    "Please select which candidate policy best matches the patient's situation.\n",
    "\n",
    "Patient Information:\n",
    "{patient_info}\n",
    "\n",
    "Candidate Policies:\"\"\"\n",
    "\n",
    "        for idx, text in enumerate(candidate_texts, 1):\n",
    "            prompt += f\"\\n\\nPolicy {idx}:\\n{text}\"\n",
    "\n",
    "        prompt += \"\"\"\n",
    "\n",
    "Please answer with only the number of the most appropriate policy.\n",
    "Do not explain. Just output the number.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "\n",
    "        if self.llm_model == \"chatgpt\":\n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=messages,\n",
    "                temperature=0\n",
    "            )\n",
    "            result = response.choices[0].message.content.strip()\n",
    "        elif self.llm_model == \"perplexity\":\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {self.perplexity_api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            data = {\n",
    "                \"model\": \"sonar-pro\",\n",
    "                \"messages\": messages,\n",
    "                \"temperature\": 0\n",
    "            }\n",
    "            url = \"https://api.perplexity.ai/chat/completions\"\n",
    "            res = requests.post(url, headers=headers, json=data)\n",
    "            result = res.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported LLM model\")\n",
    "\n",
    "        match = re.search(r'(\\d+)', result)\n",
    "        selected_idx = int(match.group(1)) - 1 if match else 0\n",
    "        return candidates[selected_idx]\n",
    "\n",
    "    def find_policy(self, patient_info):\n",
    "        insurance, test = self.extract_insurance_and_test(patient_info)\n",
    "        print(f\"üìå LLM-detected insurance: {insurance}, test: {test}\")\n",
    "\n",
    "        candidates = self.find_top_policies(patient_info, insurance)\n",
    "        print(\"üìã Top candidates:\")\n",
    "        for doc_name, score, _ in candidates:\n",
    "            print(f\"- {doc_name}: {score:.4f}\")\n",
    "\n",
    "        best_policy = self.rerank_policies(patient_info, candidates)\n",
    "        return best_policy[0], best_policy[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69dfc24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QnAExecutor:\n",
    "    def __init__(self, questions_list, llm_model=\"chatgpt\", openai_client=None, perplexity_api_key=None):\n",
    "        self.questions_list = questions_list\n",
    "        self.formatted_questions = self.format_questions()\n",
    "        self.llm_model = llm_model\n",
    "        self.openai_client = openai_client\n",
    "        self.perplexity_api_key = perplexity_api_key\n",
    "\n",
    "    def format_question_block(self, q, indent=2):\n",
    "        indent_str = \" \" * indent\n",
    "        question_line = f\"{q['question']}\"\n",
    "    \n",
    "        if q.get(\"options\") == [\"Free text answer\"]:\n",
    "            question_line += f\"\\n{indent_str}(Free text answer allowed.)\"\n",
    "        else:\n",
    "            question_line += f\"\\n{indent_str}Options: {', '.join(q['options'])}\"\n",
    "\n",
    "            if \"additional_if_yes\" in q:\n",
    "                question_line += f\"\\n{indent_str}  If 'Yes':\"\n",
    "                for item in q[\"additional_if_yes\"]:\n",
    "                    if isinstance(item, str):\n",
    "                        question_line += f\"\\n{indent_str}    - {item}\"\n",
    "                    elif isinstance(item, dict):\n",
    "                        question_line += f\"\\n{indent_str}    - {self.format_question_block(item, indent + 6)}\"\n",
    "\n",
    "            if \"additional_if_no\" in q:\n",
    "                question_line += f\"\\n{indent_str}  If 'No':\"\n",
    "                for item in q[\"additional_if_no\"]:\n",
    "                    if isinstance(item, str):\n",
    "                        question_line += f\"\\n{indent_str}    - {item}\"\n",
    "                    elif isinstance(item, dict):\n",
    "                        question_line += f\"\\n{indent_str}    - {self.format_question_block(item, indent + 6)}\"\n",
    "\n",
    "        return question_line\n",
    "\n",
    "\n",
    "    def format_questions(self):\n",
    "        return \"\\n\\n\".join([\n",
    "            f\"{q['id']}. {self.format_question_block(q)}\"\n",
    "            for q in self.questions_list\n",
    "        ])\n",
    "\n",
    "\n",
    "    def clean_json_response(self, response_text):\n",
    "        original = response_text.strip()\n",
    "\n",
    "        # Step 0: Check for hallucinated greeting (Perplexity fallback)\n",
    "        if \"how can I assist you\" in original.lower() or \"insurance-related questions\" in original.lower():\n",
    "            raise ValueError(\"Perplexity returned generic assistant response instead of JSON.\")\n",
    "\n",
    "        # Step 1: Try direct parsing\n",
    "        try:\n",
    "            return json.loads(original)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "        # Step 2: Remove code block wrappers\n",
    "        cleaned = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", original, flags=re.IGNORECASE).strip()\n",
    "        try:\n",
    "            return json.loads(cleaned)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "        # Step 3: Try to extract the first {...} JSON-like block\n",
    "        match = re.search(r\"(\\{[\\s\\S]*?\\})\", original)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(1))\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "\n",
    "        raise ValueError(\"No valid JSON found in the response.\")\n",
    "\n",
    "    def run_qna(self, patient_info, policy_name, policy_text, case_id, retrieval_model, qna_model):\n",
    "        prompt = f\"\"\"\n",
    "You are a clinical insurance assistant specializing in genetic testing coverage policies.\n",
    "\n",
    "You will be given:\n",
    "\n",
    "1. Patient clinical information (including their insurance provider, plan type, and state of residence).\n",
    "2. Official insurance policy document text (strictly use this policy content for insurance coverage decision making).\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- Answer all questions strictly based on the insurance policy document provided.\n",
    "- Do NOT refer to general guidelines or policies from other insurance providers.\n",
    "- If policy document does not clearly specify rules, you MAY use patient's clinical information to infer answers carefully.\n",
    "- Do NOT assume coverage criteria from other insurers or general clinical guidelines unless explicitly stated in the policy.\n",
    "- Output answers in JSON format ONLY.\n",
    "\n",
    "==== PATIENT INFORMATION ====\n",
    "{patient_info}\n",
    "\n",
    "==== INSURANCE POLICY DOCUMENT (from URL: {policy_name}) ====\n",
    "{policy_text}\n",
    "\n",
    "==== QUESTIONS ====\n",
    "{self.formatted_questions}\n",
    "\n",
    "Output your answers in JSON format only and include the policy_url at the end.\n",
    "\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a clinical insurance assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "\n",
    "        if self.llm_model == \"chatgpt\":\n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=messages,\n",
    "                temperature=0\n",
    "            )\n",
    "            result_content = response.choices[0].message.content.strip()\n",
    "\n",
    "        elif self.llm_model == \"perplexity\":\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {self.perplexity_api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            data = {\n",
    "                \"model\": \"sonar-pro\",\n",
    "                \"messages\": messages,\n",
    "                \"temperature\": 0\n",
    "            }\n",
    "            url = \"https://api.perplexity.ai/chat/completions\"\n",
    "            res = requests.post(url, headers=headers, json=data)\n",
    "            result_content = res.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported LLM model\")\n",
    "\n",
    "        result_json = {}\n",
    "\n",
    "        try:\n",
    "            result_json = self.clean_json_response(result_content)\n",
    "\n",
    "            final_result = {}\n",
    "            for k, v in result_json.items():\n",
    "                if k == \"policy_url\":\n",
    "                    continue\n",
    "                if \"_selection\" in k or \"_details\" in k:\n",
    "                    base_key = k.replace(\"_selection\", \"\").replace(\"_details\", \"\")\n",
    "                    final_result[f\"{base_key}_followup\"] = [v] if isinstance(v, str) else v\n",
    "                else:\n",
    "                    final_result[k] = v\n",
    "\n",
    "            folder_name = f\"{retrieval_model}_{qna_model}\"\n",
    "            save_dir = f\"/home/cptaswadu/RESCUE-n8n/insurance/results/LLM_QnA/RAG/{folder_name}\"\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            filename = os.path.join(save_dir, f\"{case_id}_qna_result.json\")\n",
    "            with open(filename, \"w\") as f:\n",
    "                json.dump(final_result, f, indent=2)\n",
    "\n",
    "            print(f\"‚úÖ QnA result saved to {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"‚ùó JSON parsing error:\", e)\n",
    "            final_result = {\n",
    "                \"error\": \"JSON parsing failed\",\n",
    "                \"raw_content\": result_content\n",
    "            }\n",
    "\n",
    "        print(\"QnA Result JSON:\", final_result)\n",
    "        return final_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "128f07d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_file_path = \"/home/cptaswadu/RESCUE-n8n/insurance/dataset/Insurance_Genetic_Testing_QA.json\"\n",
    "\n",
    "with open(questions_file_path, \"r\") as f:\n",
    "    questions_data = json.load(f)\n",
    "\n",
    "questions_list = questions_data[\"questions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69724405",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_ex = [\n",
    "    {\n",
    "        \"id\": \"Case1\",\n",
    "        \"patient_info\": \"An 8-year-old boy with neurodevelopmental delay and seizures. A prior chromosomal microarray test was negative. Whole exome sequencing (WES) has been requested by the genetic counselor to investigate potential underlying genetic causes that may guide diagnosis and future treatment decisions. There is also a family history of neurodevelopmental disorders, as his older brother was diagnosed with autism spectrum disorder. The patient is covered by United Healthcare Choice Plus through a family plan and resides in New Jersey.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b3cb1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Running: chatgpt_chatgpt\n",
      "‚úÖ Loaded 585 policies.\n",
      "‚úÖ Embeddings created.\n",
      "üìå LLM-detected insurance: United Healthcare, test: Whole Exome Sequencing\n",
      "üìã Top candidates:\n",
      "- United Healthcare_whole-exome-and-whole-genome-sequencing.pdf: 0.4831\n",
      "- United Healthcare_genetic-testing-neuromuscular-disorders.pdf: 0.4247\n",
      "- United Healthcare_genetic-testing-hereditary-cancer.pdf: 0.4086\n",
      "- United Healthcare_preimplantation-genetic-testing.pdf: 0.3903\n",
      "- United Healthcare_genetic-testing-cardiac-disease.pdf: 0.3890\n",
      "‚úÖ QnA result saved to /home/cptaswadu/RESCUE-n8n/insurance/results/LLM_QnA/RAG/chatgpt_chatgpt/Case1_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'Whole Exome Sequencing (WES)', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'No', 'Q5': 'Yes', 'Q6': 'No', 'Q7': 'Yes', 'Q8': 'No', 'Q9': 'Yes', 'Q10': 'No', 'Q11': 'Yes', 'Q12': 'Yes', 'Q13': 'Not specified', 'Q14': 'Yes', 'Q14_followup': ['81415'], 'Q15': 'No', 'Q16': 'Yes', 'Q17': 'To submit the claim, ensure that the test was ordered by an approved specialist, confirm that the patient meets the medical necessity criteria, and include documentation of prior genetic testing results. Genetic counseling should be documented as provided. Verify if prior authorization is required and obtain it if necessary.'}\n",
      "\n",
      "üöÄ Running: chatgpt_perplexity\n",
      "‚úÖ Loaded 585 policies.\n",
      "‚úÖ Embeddings created.\n",
      "üìå LLM-detected insurance: United Healthcare, test: Whole Exome Sequencing\n",
      "üìã Top candidates:\n",
      "- United Healthcare_whole-exome-and-whole-genome-sequencing.pdf: 0.4831\n",
      "- United Healthcare_genetic-testing-neuromuscular-disorders.pdf: 0.4247\n",
      "- United Healthcare_genetic-testing-hereditary-cancer.pdf: 0.4086\n",
      "- United Healthcare_preimplantation-genetic-testing.pdf: 0.3903\n",
      "- United Healthcare_genetic-testing-cardiac-disease.pdf: 0.3890\n",
      "‚úÖ QnA result saved to /home/cptaswadu/RESCUE-n8n/insurance/results/LLM_QnA/RAG/chatgpt_perplexity/Case1_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'Whole exome sequencing (WES) has been requested for an 8-year-old boy with neurodevelopmental delay and seizures. A prior chromosomal microarray test was negative. The test was ordered by a genetic counselor to investigate potential underlying genetic causes.', 'Q1': 'Yes', 'Q2': 'No', 'Q3': 'Yes', 'Q4': 'Yes', 'Q4_1': 'Yes', 'Q4_1_1': 'ACMG', 'Q5': 'Yes', 'Q6': 'No', 'Q7': 'Yes', 'Q8': 'No', 'Q9': 'Yes', 'Q9_1': 'Yes', 'Q10': 'No', 'Q10_1': 'Diagnostic', 'Q11': 'Not specified', 'Q12': 'Yes', 'Q13': 'Not specified', 'Q14': 'Yes', 'Q14_1': '81415 (Exome sequence analysis)', 'Q15': 'No', 'Q16': 'No', 'Q17': \"According to the policy, WES must be ordered by a medical geneticist, neonatologist, neurologist, or developmental pediatrician. In this case, the test was ordered by a genetic counselor, which does not meet the policy requirements. To submit a claim that would be covered, the test would need to be ordered by one of the approved specialists. Additionally, documentation should include the patient's clinical history, prior testing results (including the negative chromosomal microarray), and explanation of how the results will influence medical management and clinical outcomes.\"}\n",
      "\n",
      "üöÄ Running: perplexity_chatgpt\n",
      "‚úÖ Loaded 585 policies.\n",
      "‚úÖ Embeddings created.\n",
      "üìå LLM-detected insurance: United Healthcare Choice Plus, test: Whole Exome Sequencing (WES)\n",
      "‚ùó No policies matched the insurance. Using all policies.\n",
      "üìã Top candidates:\n",
      "- CapitalBC_Developmental delay.pdf: 0.6820\n",
      "- V2.2023 Concert Genetic Testing- Exome & Genome Sequencing for Dx of Genetic Disorders.pdf: 0.6691\n",
      "- BCBS_NE_V36.pdf: 0.6378\n",
      "- Dean_HealthPlan_Genetic-Testing-Exome-Genome-9586.pdf: 0.6326\n",
      "- Medica_Genetic-Testing-for-Exome-and-Genome-Sequencing-for-the-Diagnosis-of-Genetic-Disorders.pdf: 0.6268\n",
      "‚úÖ QnA result saved to /home/cptaswadu/RESCUE-n8n/insurance/results/LLM_QnA/RAG/perplexity_chatgpt/Case1_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'Whole Exome Sequencing (WES)', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q4_followup': {'match': 'Yes', 'guidelines': ['ACMG']}, 'Q5': 'Yes', 'Q6': 'No', 'Q7': 'Yes', 'Q8': 'No', 'Q9': 'Yes', 'Q9_followup': ['Yes'], 'Q10': 'No', 'Q10_followup': ['Diagnostic'], 'Q11': 'Yes', 'Q12': 'No', 'Q13': 'No', 'Q14': 'Yes', 'Q14_followup': ['81415, 81416'], 'Q15': 'No', 'Q16': 'Yes', 'Q17': \"Submit the claim with the appropriate CPT codes (81415, 81416) and ensure that documentation of medical necessity, including the patient's clinical presentation and family history, is included. Genetic counseling documentation should also be provided.\"}\n",
      "\n",
      "üöÄ Running: perplexity_perplexity\n",
      "‚úÖ Loaded 585 policies.\n",
      "‚úÖ Embeddings created.\n",
      "üìå LLM-detected insurance: United Healthcare Choice Plus, test: Whole Exome Sequencing (WES)\n",
      "‚ùó No policies matched the insurance. Using all policies.\n",
      "üìã Top candidates:\n",
      "- CapitalBC_Developmental delay.pdf: 0.6820\n",
      "- V2.2023 Concert Genetic Testing- Exome & Genome Sequencing for Dx of Genetic Disorders.pdf: 0.6691\n",
      "- BCBS_NE_V36.pdf: 0.6378\n",
      "- Dean_HealthPlan_Genetic-Testing-Exome-Genome-9586.pdf: 0.6326\n",
      "- Medica_Genetic-Testing-for-Exome-and-Genome-Sequencing-for-the-Diagnosis-of-Genetic-Disorders.pdf: 0.6268\n",
      "‚úÖ QnA result saved to /home/cptaswadu/RESCUE-n8n/insurance/results/LLM_QnA/RAG/perplexity_perplexity/Case1_qna_result.json\n",
      "QnA Result JSON: {'Q0': 'Whole exome sequencing (WES) to investigate potential underlying genetic causes of neurodevelopmental delay and seizures. Specific gene content is not specified, as WES covers all protein-coding regions of the genome.', 'Q1': 'Yes', 'Q2': 'Yes', 'Q3': 'Yes', 'Q4': 'Yes', 'Q4a': 'Yes', 'Q4b': ['ACMG'], 'Q5': 'Yes', 'Q6': 'No', 'Q7': 'Yes', 'Q8': 'No', 'Q9': 'Yes', 'Q9a': 'Yes', 'Q10': 'No', 'Q10a': 'Diagnostic', 'Q11': 'Yes', 'Q12': 'Yes', 'Q13': 'Not specified', 'Q14': 'Yes', 'Q14a': ['81415', '81416', '0214U', '0215U'], 'Q15': 'No', 'Q16': 'Yes', 'Q17': 'To submit the claim, ensure that: (1) the test is ordered by an appropriate provider (e.g., Board-Certified Medical Geneticist or Certified Genetic Counselor); (2) documentation includes evidence of prior negative chromosomal microarray, clinical features, and family history; (3) pre- and post-test counseling is documented; (4) CPT codes 81415, 81416, 0214U, or 0215U are used as appropriate; (5) all required clinical notes and genetic counseling documentation are included. Prior authorization may be required‚Äîverify with the health plan before proceeding.'}\n"
     ]
    }
   ],
   "source": [
    "models = [\"chatgpt\", \"perplexity\"]\n",
    "model_map = {\"chatgpt\": \"chatgpt\", \"perplexity\": \"perplexity\"}\n",
    "model_combinations = [(r, q) for r in models for q in models]\n",
    "       \n",
    "\n",
    "for retrieval_model, qna_model in model_combinations:\n",
    "    print(f\"\\nüöÄ Running: {retrieval_model}_{qna_model}\")\n",
    "\n",
    "    retriever = RAGPolicyRetriever(\n",
    "        policy_folder_path=\"/home/cptaswadu/RESCUE-n8n/insurance/insurance_policy\",\n",
    "        openai_api_key=openai_api_key,\n",
    "        perplexity_api_key=perplexity_api_key,\n",
    "        llm_model=model_map[retrieval_model]\n",
    "    )\n",
    "    retriever.load_policies()\n",
    "    retriever.embed_policies()\n",
    "\n",
    "    executor = QnAExecutor(\n",
    "        questions_list=questions_list,\n",
    "        llm_model=model_map[qna_model],\n",
    "        openai_client=retriever.openai_client,\n",
    "        perplexity_api_key=perplexity_api_key\n",
    "    )\n",
    "\n",
    "    for case in case_ex:\n",
    "        case_id = case[\"id\"]\n",
    "        patient_info = case[\"patient_info\"]\n",
    "\n",
    "        try:\n",
    "            policy_name, policy_text = retriever.find_policy(patient_info)\n",
    "            executor.run_qna(\n",
    "                patient_info=patient_info,\n",
    "                policy_name=policy_name,\n",
    "                policy_text=policy_text,\n",
    "                case_id=case_id,\n",
    "                retrieval_model=retrieval_model,\n",
    "                qna_model=qna_model  \n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed on {case_id} ({retrieval_model}_{qna_model}): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8499373d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Merged CSV saved to: /home/cptaswadu/RESCUE-n8n/insurance/results/LLM_QnA/RAG/chatgpt_perplexity.csv\n",
      "‚úÖ Merged CSV saved to: /home/cptaswadu/RESCUE-n8n/insurance/results/LLM_QnA/RAG/chatgpt_chatgpt.csv\n",
      "‚úÖ Merged CSV saved to: /home/cptaswadu/RESCUE-n8n/insurance/results/LLM_QnA/RAG/perplexity_chatgpt.csv\n",
      "‚úÖ Merged CSV saved to: /home/cptaswadu/RESCUE-n8n/insurance/results/LLM_QnA/RAG/perplexity_perplexity.csv\n"
     ]
    }
   ],
   "source": [
    "def merge_qna_jsons_to_csv(folder_path, output_csv_path):\n",
    "    all_data = []\n",
    "\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\"_qna_result.json\"):\n",
    "            case_id = file.replace(\"_qna_result.json\", \"\")\n",
    "            json_path = os.path.join(folder_path, file)\n",
    "\n",
    "            with open(json_path, \"r\") as f:\n",
    "                try:\n",
    "                    result = json.load(f)\n",
    "                    flat_result = {\"case_id\": case_id}\n",
    "\n",
    "                    for k, v in result.items():\n",
    "                        if isinstance(v, list):\n",
    "                            flat_result[k] = \"; \".join(map(str, v))\n",
    "                        else:\n",
    "                            flat_result[k] = v\n",
    "\n",
    "                    all_data.append(flat_result)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùó Failed to parse {file}: {e}\")\n",
    "\n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data)\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "        print(f\"‚úÖ Merged CSV saved to: {output_csv_path}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No valid QnA result files found in: {folder_path}\")\n",
    "\n",
    "def merge_all_combinations_to_csv(base_dir):\n",
    "    for folder_name in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            output_csv = os.path.join(base_dir, f\"{folder_name}.csv\")\n",
    "            merge_qna_jsons_to_csv(folder_path, output_csv)\n",
    "\n",
    "\n",
    "merge_all_combinations_to_csv(\n",
    "    base_dir=\"/home/cptaswadu/RESCUE-n8n/insurance/results/LLM_QnA/RAG\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce96f0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = {\n",
    "  \"Case1\": {\n",
    "    \"Q0\": \"Whole Exome Sequencing (WES)\",\n",
    "    \"Q1\": \"Yes\",\n",
    "    \"Q2\": \"Yes\",\n",
    "    \"Q3\": \"Yes\",\n",
    "    \"Q4\": \"Yes\",\n",
    "    \"Q4_followup\": [\n",
    "      {\n",
    "        \"answer\": \"Yes\",\n",
    "        \"followup\": [\"ACMG\"]\n",
    "      }\n",
    "    ],\n",
    "    \"Q5\": \"Yes\",\n",
    "    \"Q6\": \"No\",\n",
    "    \"Q7\": \"Yes\",\n",
    "    \"Q8\": \"No\",\n",
    "    \"Q9\": \"Yes\",\n",
    "    \"Q9_followup\": [\"Yes\"],\n",
    "    \"Q10\": \"No\",\n",
    "    \"Q10_followup\": [\"Diagnostic\"],\n",
    "    \"Q11\": \"Yes\",\n",
    "    \"Q12\": \"No\",\n",
    "    \"Q13\": \"Yes\",\n",
    "    \"Q14\": \"Yes\",\n",
    "    \"Q14_followup\": [\"81415\", \"81416\"],\n",
    "    \"Q15\": \"No\",\n",
    "    \"Q16\": \"Yes\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "127b0087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_converted_results_from_folder(folder_path):\n",
    "    converted_results = {}\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\"_qna_result.json\"):\n",
    "            case_id = file.replace(\"_qna_result.json\", \"\")\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            with open(file_path, \"r\") as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"‚ùó JSON decode error in {file}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                clean = {}\n",
    "                for k, v in data.items():\n",
    "                    if k.endswith(\"_followup\"):\n",
    "                        clean[k] = v if isinstance(v, list) else [v]\n",
    "                    else:\n",
    "                        clean[k] = v\n",
    "\n",
    "                converted_results[case_id] = clean\n",
    "    return converted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55011b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_qna_result(case_id, predicted_result, gold_result, folder_path=None):\n",
    "    records = []\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for qid in gold_result:\n",
    "        if not qid.startswith(\"Q\") or qid == \"policy_url\" or qid == \"Q17\" or \"_followup\" in qid:\n",
    "            continue\n",
    "\n",
    "        pred_answer = predicted_result.get(qid, \"\")\n",
    "        gold_answer = gold_result.get(qid, \"\")\n",
    "\n",
    "        if isinstance(pred_answer, list):\n",
    "            pred_answer = \", \".join(pred_answer)\n",
    "        if isinstance(gold_answer, list):\n",
    "            gold_answer = \", \".join(gold_answer)\n",
    "\n",
    "        pred_answer = pred_answer.strip()\n",
    "        gold_answer = gold_answer.strip()\n",
    "\n",
    "        is_correct = pred_answer == gold_answer\n",
    "        score = 1 if is_correct else 0\n",
    "\n",
    "        records.append({\n",
    "            \"Case\": case_id,\n",
    "            \"Question\": qid,\n",
    "            \"Predicted\": pred_answer,\n",
    "            \"Gold\": gold_answer,\n",
    "            \"Score\": score\n",
    "        })\n",
    "\n",
    "        total_count += 1\n",
    "        correct_count += score\n",
    "\n",
    "        followup_key = qid + \"_followup\"\n",
    "        pred_followup = predicted_result.get(followup_key, None)\n",
    "        gold_followup = gold_result.get(followup_key, None)\n",
    "\n",
    "        if is_correct and gold_followup is not None:\n",
    "            def normalize(ans):\n",
    "                if ans is None:\n",
    "                    return \"None\"\n",
    "                if isinstance(ans, list):\n",
    "                    return \", \".join([a if isinstance(a, str) else a.get(\"answer\", str(a)) for a in ans])\n",
    "                return ans if isinstance(ans, str) else str(ans)\n",
    "\n",
    "            pred_followup_norm = normalize(pred_followup)\n",
    "            gold_followup_norm = normalize(gold_followup)\n",
    "\n",
    "            followup_score = 1 if pred_followup_norm == gold_followup_norm else 0\n",
    "\n",
    "            records.append({\n",
    "                \"Case\": case_id,\n",
    "                \"Question\": followup_key,\n",
    "                \"Predicted\": pred_followup_norm,\n",
    "                \"Gold\": gold_followup_norm,\n",
    "                \"Score\": followup_score\n",
    "            })\n",
    "\n",
    "            total_count += 1\n",
    "            correct_count += followup_score\n",
    "\n",
    "    accuracy = correct_count / total_count * 100 if total_count > 0 else 0\n",
    "\n",
    "    records.append({\n",
    "        \"Case\": case_id,\n",
    "        \"Question\": \"TOTAL\",\n",
    "        \"Predicted\": f\"Correct: {correct_count}\",\n",
    "        \"Gold\": f\"Incorrect: {total_count - correct_count}\",\n",
    "        \"Score\": f\"Accuracy: {accuracy:.2f}%\"\n",
    "    })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    # ‚úÖ Save CSV if folder_path is given\n",
    "    if folder_path:\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        csv_path = os.path.join(folder_path, f\"evaluation_{case_id}.csv\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"üìÑ Saved evaluation to {csv_path}\")\n",
    "\n",
    "    return df, accuracy\n",
    "\n",
    "    return pd.DataFrame(records), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "473725f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_folders_with_summary(base_dir, gold_answers, summary_output_csv):\n",
    "    eval_output_dir = os.path.join(base_dir, \"Evaluation\")\n",
    "    os.makedirs(eval_output_dir, exist_ok=True)\n",
    "\n",
    "    summary_records = []\n",
    "\n",
    "    for folder_name in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder_name)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüìä Evaluating folder: {folder_name}\")\n",
    "\n",
    "        converted_results = load_converted_results_from_folder(folder_path)\n",
    "        all_dfs = []\n",
    "        accuracies = []\n",
    "\n",
    "        for case_id, pred_result in converted_results.items():\n",
    "            gold_result = gold_answers.get(case_id)\n",
    "            if gold_result is None:\n",
    "                continue\n",
    "\n",
    "            df_case, acc = evaluate_qna_result(case_id, pred_result, gold_result)\n",
    "            all_dfs.append(df_case)\n",
    "            accuracies.append(acc)\n",
    "\n",
    "        if all_dfs:\n",
    "            merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "            eval_csv_path = os.path.join(folder_path, \"evaluation_results.csv\")  # ‚¨ÖÔ∏è Save inside each folder\n",
    "            merged_df.to_csv(eval_csv_path, index=False)\n",
    "            print(f\"‚úÖ Saved: {eval_csv_path}\")\n",
    "\n",
    "            summary_records.append({\n",
    "                \"Model_Combination\": folder_name,\n",
    "                \"Mean_Accuracy\": f\"{sum(accuracies) / len(accuracies):.2f}%\" if accuracies else \"N/A\"\n",
    "            })\n",
    "\n",
    "    if summary_records:\n",
    "        summary_df = pd.DataFrame(summary_records)\n",
    "        os.makedirs(os.path.dirname(summary_output_csv), exist_ok=True)\n",
    "        summary_df.to_csv(summary_output_csv, index=False)\n",
    "        print(f\"\\n‚úÖ Summary saved to: {summary_output_csv}\")\n",
    "        print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70764f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluating folder: chatgpt_perplexity\n",
      "‚úÖ Saved: /home/cptaswadu/RESCUE-n8n/insurance/results/LLM_QnA/RAG/chatgpt_perplexity/evaluation_results.csv\n",
      "\n",
      "üìä Evaluating folder: Evaluation\n",
      "\n",
      "üìä Evaluating folder: chatgpt_chatgpt\n",
      "‚úÖ Saved: /home/cptaswadu/RESCUE-n8n/insurance/results/LLM_QnA/RAG/chatgpt_chatgpt/evaluation_results.csv\n",
      "\n",
      "üìä Evaluating folder: perplexity_chatgpt\n",
      "‚úÖ Saved: /home/cptaswadu/RESCUE-n8n/insurance/results/LLM_QnA/RAG/perplexity_chatgpt/evaluation_results.csv\n",
      "\n",
      "üìä Evaluating folder: perplexity_perplexity\n",
      "‚úÖ Saved: /home/cptaswadu/RESCUE-n8n/insurance/results/LLM_QnA/RAG/perplexity_perplexity/evaluation_results.csv\n",
      "\n",
      "‚úÖ Summary saved to: /home/cptaswadu/RESCUE-n8n/insurance/results/LLM_QnA/RAG/Evaluation/summary_accuracy.csv\n",
      "       Model_Combination Mean_Accuracy\n",
      "0     chatgpt_perplexity        52.38%\n",
      "1        chatgpt_chatgpt        70.00%\n",
      "2     perplexity_chatgpt        90.48%\n",
      "3  perplexity_perplexity        66.67%\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"/home/cptaswadu/RESCUE-n8n/insurance/results/LLM_QnA/RAG\"\n",
    "summary_output_csv = os.path.join(base_dir, \"Evaluation\", \"summary_accuracy.csv\")\n",
    "\n",
    "evaluate_all_folders_with_summary(\n",
    "    base_dir=base_dir,\n",
    "    gold_answers=ground_truth,\n",
    "    summary_output_csv=summary_output_csv\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
